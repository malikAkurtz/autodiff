import numpy as np
from Tensor import Tensor
from NeuralNetwork import NeuralNetwork
from Layer import Layer
from Node import Node
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def cost_fn(y_preds_tensor: Tensor, y_true_tensor: Tensor):
    num_elements = y_preds_tensor.shape[0]
    return ((y_preds_tensor - y_true_tensor) ** 2).sum(axis=None) / num_elements


def main():
    # Build data distribution
    X = np.linspace(0, 10, 100)
    X = X / 10
    y = np.sin(X)
    
    # Split data into test, train
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # Build initial model
    L0_weights = np.array([
        [0.1, 0.1, 0.1]
    ])
    L0_bias = np.array([1.0, 1.0, 1.0])
    
    L0_weights_tensor = Tensor(L0_weights, requires_gradient=True)
    L0_bias_tensor = Tensor(L0_bias, requires_gradient=True)
    L0 = Layer(weights_tensor=L0_weights_tensor, bias_tensor=L0_bias_tensor, activation=Tensor.ReLU)
    
    L1_weights = np.array([
        [0.1], 
        [0.1], 
        [0.1]
    ])
    L1_bias = np.array([1.0])
    
    L1_weights_tensor = Tensor(L1_weights, requires_gradient=True)
    L1_bias_tensor = Tensor(L1_bias, requires_gradient=True)
    L1 = Layer(weights_tensor=L1_weights_tensor, bias_tensor=L1_bias_tensor, activation=None)
    
    network = NeuralNetwork([L0, L1])
    
    # Run gradient descent
    num_epochs = 1000
    batch_size = 16 
    learning_rate = 0.01
    
    history = {"train_loss": []}
    
    # For each epoch (for each run through the training data)
    for _ in range(num_epochs):
        # For each batch
        epoch_avg_loss = 0
        batch_counter = 0
        for x_batch, y_batch in batches(x_train, y_train, batch_size):
            # x_batch has shape (batch_size,)
            # need to reshape to (batch_size, 1)
            x_batch = x_batch.reshape(-1, 1)
            # Cast to Tensor
            x_batch_tensor = Tensor(x_batch, requires_gradient=False)
            print("x_batch:")
            print(x_batch_tensor)
            
            # y_batch has shape(batch_size,)
            # need to reshape to (batch_size, 1)
            y_batch = y_batch.reshape(-1, 1)
            # Cast to Tensor
            y_batch_tensor = Tensor(y_batch, requires_gradient=False)
            print(f"y_batch:")
            print(y_batch_tensor)
            
            # Perform forward pass to create computational graph
            batch_output_tensor = network.forward(x_batch_tensor)
            print(f"Batch output:")
            print(batch_output_tensor)
            
            # Calculate cost
            batch_loss = cost_fn(batch_output_tensor, y_batch_tensor)
            print(f"Epoch {_} Batch {batch_counter} Loss: {batch_loss.data}")
            batch_counter += 1
            epoch_avg_loss += batch_loss.data
            
            # Perform backward pass
            batch_loss.backward()
            
            # Take GD step
            for layer in network.layers:
                layer.weights_tensor.data -= learning_rate * layer.weights_tensor.grad
                layer.bias_tensor.data    -= learning_rate * layer.bias_tensor.grad
                
            # Zero gradients
            for layer in network.layers:
                layer.weights_tensor.grad = np.zeros_like(layer.weights_tensor.data)
                layer.bias_tensor.grad = np.zeros_like(layer.bias_tensor.data)
                
        epoch_avg_loss /= batch_size
        history["train_loss"].append(epoch_avg_loss)
        
    # Run inference on test data
    # Reshape x_test, y_test
    x_test = x_test.reshape(-1, 1)
    y_test = y_test.reshape(-1, 1)
    # Cast x_test, y_test to Tensors
    x_test_tensor = Tensor(x_test, requires_gradient=False)
    y_test_tensor = Tensor(y_test, requires_gradient=False)
    y_preds_tensor = network.forward(x_test_tensor)
    test_loss_tensor = cost_fn(y_preds_tensor, y_test_tensor)
    
    print(f"Loss on Test Data: {test_loss_tensor.data}")
    plot_history(history, "Training Loss")
    plot_data(x_test_tensor.data, y_test_tensor.data, y_preds_tensor.data)
        
        
def plot_history(history, plot_title='', save_path=None):
    epochs = range(len(history["train_loss"]))
    plt.plot(epochs, history["train_loss"], color="blue")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(plot_title)
    plt.legend()
    plt.grid(True)
    plt.show()
    
def plot_data(x_test, y_test, y_preds):
    plt.scatter(x_test, y_test, color="blue")
    plt.scatter(x_test, y_preds, color="red")
    plt.xlabel("X Value")
    plt.ylabel("Y Value")
    plt.legend()
    plt.grid(True)
    plt.show()
            
def batches(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i+batch_size], y[i:i+batch_size]
    

if __name__=="__main__":
    main()from __future__ import annotations
import numpy as np

from Tensor import Tensor
from Node import Node
from debug import DEBUG


class Layer:
    global_id = 0
    def __init__(self, weights_tensor: Tensor, bias_tensor: Tensor, activation: callable):
        self.weights_tensor = weights_tensor
        self.bias_tensor = bias_tensor
        self.activation = activation
        self._id = Layer.global_id
        Layer.global_id += 1
    import numpy as np
from Tensor import Tensor
from Layer import Layer
from Node import Node
from debug import DEBUG

class NeuralNetwork:
    def __init__(self, layers: list[Layer] = None):
        self.layers = layers
    
    def forward(self, batch_input_tensor: Tensor):
        output_tensor = batch_input_tensor
        
        for layer in self.layers:
            output_tensor = output_tensor @ layer.weights_tensor
            output_tensor = output_tensor + layer.bias_tensor
            if layer.activation:
                output_tensor = layer.activation(output_tensor)
            if DEBUG:
                print(f"Layer {layer._id} output_tensor:")
                print(output_tensor)
        
        return output_tensor
    
        from __future__ import annotations
import numpy as np
from debug import DEBUG, SUPER_DEBUG

class Tensor:
    # TODO: Only propogate gradients to Tensors with requires_gradient=True
    # TODO: store .grad as a Tensor s.t. we can compute higher order derivatives
    global_id = 0
    
    def __init__(self, data: np.array, requires_gradient: bool):
        # The raw Tensor data, stored as an n-dimensional nd.array
        self.data = data
        # The shape of the Tensor, i.e. the shape of the n-dimensional nd.array
        self.shape = data.shape
        """
        Boolean denoting whether we want to calculate the partial derivative
        of the overall function with respect to this Tensor
        e.g if we have input data Tensor, this is just data (not a parameter)
        so we dont need to calculate the tensors gradient
        """
        self.requires_gradient = requires_gradient
        # The Tensors used to produce this Tensor
        self._parents = []
        # The function that propogates this Tensor's 
        # gradient to its parents
        self._backward = None
        self._backward_metadata = None
        # The gradient of the overall function 
        # with respect to this Tensor
        self.grad = None
        # id used to identify the Tensor for debugging
        self._id = Tensor.global_id
        Tensor.global_id += 1
    
    def __str__(self):
        header = f"------ Tensor {self._id} ------\n"
        data = str(self.data) + "\n"
        shape = f"Shape: {self.shape}\n"
        footer = "-------------------------------"
        return header + data + shape + footer

    
    # Helper function to set the parents of this Tensor object
    # given its parent Tensors
    def set_parents(self, parents: list[Tensor]):
        self._parents = parents
        
    def backward(self):
        # To start the recursive process
        self.grad = np.ones_like(self.data)
        # Array to store the topological ordering of
        # Tensors in the computational graph
        topological_order = []
        """
        Set used in Post-Order DFS algorithm
        to store which nodes (Tensors) we have
        already visited
        """
        visited = set()
        
        # Post-Order DFS helper function
        def postOrderDFS(tensor: Tensor):
            # If the Tensor is None (at a leaf node)
            # or we have already visited this node
            # then return
            if tensor is None or tensor in visited:
                return
            
            # Otherwise visit the Tensor
            visited.add(tensor)
            
            # For every parent of this Tensor
            for parent_tensor in tensor._parents:
                # Run Post-Order DFS on it
                postOrderDFS(parent_tensor)
            
            # After running DFS on all parents, we have ensured that all
            # dependencies of this tensor (the nodes that it depends on)
            # appear earlier in the topological order.
            # So now we can safely add this tensor itself.
            topological_order.append(tensor)
            
        postOrderDFS(self)
        
        if DEBUG:
            print("Forward Topological Ordering (Parent -> Child):")
            for tensor in topological_order:
                print(tensor._id)
        
        for tensor in reversed(topological_order):
            if tensor._backward is not None:
                tensor._backward()
    
    def as_Tensor(x):
        if isinstance(x, Tensor):
            return x
        else:
            return Tensor(np.array(x), requires_gradient=False)
                
    def _reduce_grad_for_broadcast(child_grad: np.array, parent_shape: tuple,  child_shape: tuple):
        """
        child_grad: The gradient flowing into this parent from upstream
        parent_shape: The shape of the parent before broadcasting
        child_shape: The shape of the output child after broadcasing
        (if parent_shape != child_shape ==> broadcasing has occurred)
        """
        # parent_shape will always have smaller rank than child_shape
        # rank = # of indices required to identify an element
        rank_diff = len(child_shape) - len(parent_shape)
        parent_shape_padded = (1,) * rank_diff + parent_shape
        
        axes_to_sum = []
        # Anywhere parent_shape_padded == 1 and  child_shape != 1, a broadcast took place
        for i, (in_dim, out_dim) in enumerate(zip(parent_shape_padded, child_shape)):
            if in_dim == 1 and out_dim != 1:
                axes_to_sum.append(i)
                
        if axes_to_sum:
            child_grad = child_grad.sum(axis=tuple(axes_to_sum), keepdims=False)
                
        return child_grad.reshape(parent_shape)
        
    # Assuming we will only be working with 1D Tensors at the moment
    # i.e. only working with scalars until everything is working
    def __add__(self, other_parent: Tensor):
        # Make sure any input is a Tensor
        other_parent = Tensor.as_Tensor(other_parent)
        # Adding two numpy arrays together to produce another numpy array
        child_data = self.data + other_parent.data
        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if self.requires_gradient or other_parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False

        # The new child Tensor
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parents
        parents = [self, other_parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id} + {parents[1]._id}")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            child_grad_self = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=self.shape,
                                                          child_shape=child.grad.shape)
            if self.grad is not None:
                self.grad += child_grad_self
            else:
                self.grad = child_grad_self
                            
            child_grad_other = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=other_parent.shape,
                                                          child_shape=child.grad.shape)
            if other_parent.grad is not None:
                other_parent.grad += child_grad_other
            else:
                other_parent.grad = child_grad_other
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                            
        child._backward = _backward
        
        return child

        
    def __sub__(self, other_parent: Tensor):
        # Make sure any input is a Tensor
        other_parent = Tensor.as_Tensor(other_parent)
        # Subtracting two numpy arrays together to produce another numpy array
        child_data = self.data - other_parent.data
        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if self.requires_gradient or other_parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False

        # The new child Tensor
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parents
        parents = [self, other_parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id} - {parents[1]._id}")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
                
            child_grad_self = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=self.shape,
                                                          child_shape=child.grad.shape)
            if self.grad is not None:
                self.grad += child_grad_self
            else:
                self.grad = child_grad_self
            
            child_grad_other = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=other_parent.shape,
                                                          child_shape=child.grad.shape)
            if other_parent.grad is not None:
                other_parent.grad -= child_grad_other
            else:
                other_parent.grad = -child_grad_other
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
        
        
    def __mul__(self, other_parent: Tensor):
        # Make sure any input is a Tensor
        other_parent = Tensor.as_Tensor(other_parent)
        # Multiplying two numpy arrays together to produce another numpy array
        child_data = self.data * other_parent.data
        
        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if self.requires_gradient or other_parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False

        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parents
        parents = [self, other_parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id} * {parents[1]._id}")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
                
            child_grad_self = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=self.shape,
                                                          child_shape=child.grad.shape)
            if self.grad is not None:
                self.grad += (other_parent.data) * child_grad_self
            else:
                self.grad = (other_parent.data) * child_grad_self
            
            child_grad_other = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=other_parent.shape,
                                                          child_shape=child.grad.shape)
            if other_parent.grad is not None:
                other_parent.grad += (self.data) * child_grad_other
            else:
                other_parent.grad = (self.data) * child_grad_other
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def __matmul__(self, other_parent: Tensor):
        # Make sure any input is a Tensor
        other_parent = Tensor.as_Tensor(other_parent)
        # Mat Multiplying two numpy arrays together to produce another numpy array
        child_data = self.data @ other_parent.data
        
        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if self.requires_gradient or other_parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False

        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parents
        parents = [self, other_parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id} @ {parents[1]._id}")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
                
            if self.grad is not None:
                self.grad += child.grad @ other_parent.data.T
            else:
                self.grad = child.grad @ other_parent.data.T
            
            if other_parent.grad is not None:
                other_parent.grad += self.data.T @ child.grad
            else:
                other_parent.grad = self.data.T @ child.grad
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
        
        
    def __truediv__(self, other_parent: Tensor):
        # Make sure any input is a Tensor
        other_parent = Tensor.as_Tensor(other_parent)
        # Dividing two numpy arrays together to produce another numpy array
        child_data = self.data / other_parent.data
        
        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if self.requires_gradient or other_parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False

        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parents
        parents = [self, other_parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id} / {parents[1]._id}")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
                
            child_grad_self = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=self.shape,
                                                          child_shape=child.grad.shape)
            if self.grad is not None:
                self.grad += child_grad_self / other_parent.data
            else:
                self.grad = child_grad_self / other_parent.data

            child_grad_other = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=other_parent.shape,
                                                          child_shape=child.grad.shape)
            if other_parent.grad is not None:
                other_parent.grad -= (child_grad_other * self.data) / (other_parent.data**2)
            else:
                other_parent.grad = -(child_grad_other * self.data) / (other_parent.data**2)
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def __pow__(self, other_parent: Tensor):
        # Make sure any input is a Tensor
        other_parent = Tensor.as_Tensor(other_parent)
        # exponentiating one numpy array by the parent to produce another numpy array
        child_data = self.data ** other_parent.data
        
        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if self.requires_gradient or other_parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False

        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parents
        parents = [self, other_parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id} ** {parents[1]._id}")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            child_grad_self = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=self.shape,
                                                          child_shape=child.grad.shape)
            if self.grad is not None:
                self.grad += child_grad_self * other_parent.data * (self.data ** (other_parent.data -  1))
            else:
                self.grad = child_grad_self * other_parent.data * (self.data ** (other_parent.data -  1))
            
            child_grad_other = Tensor._reduce_grad_for_broadcast(child_grad=child.grad, 
                                                          parent_shape=other_parent.shape,
                                                          child_shape=child.grad.shape)
            if other_parent.grad is not None:
                other_parent.grad += child_grad_other * (self.data ** other_parent.data) * np.log(self.data)
            else:
                other_parent.grad = child_grad_other * (self.data ** other_parent.data) * np.log(self.data)
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
        
        
    def exp(parent: Tensor):
        # Make sure any input is a Tensor
        parent = Tensor.as_Tensor(parent)
        # The raw tensor data, exponentiated
        child_data = np.exp(parent.data)

        # If either parent of the output Tensor requires a gradient
        # Then this output Tensor will also require a gradient
        if parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: exp({parents[0]._id})")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if parent.grad is not None:
                parent.grad += (child.grad * np.exp(parent.data))
            else:
                parent.grad = (child.grad * np.exp(parent.data))
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def sin(parent: Tensor):
        # Make sure any input is a Tensor
        parent = Tensor.as_Tensor(parent)
        # The raw tensor data, with sin applied
        child_data = np.sin(parent.data)

        # If parent of output requires gradient, child requires gradient
        if parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: sin({parents[0]._id})")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if parent.grad is not None:
                parent.grad += (child.grad * np.cos(parent.data))
            else:
                parent.grad = (child.grad * np.cos(parent.data))
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def cos(parent: Tensor):
        # Make sure any input is a Tensor
        parent = Tensor.as_Tensor(parent)
        # The raw tensor data, with cos applied
        child_data = np.cos(parent.data)

        # If parent of output requires gradient, child requires gradient
        if parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: cos({parents[0]._id})")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if parent.grad is not None:
                parent.grad += (child.grad * -np.sin(parent.data))
            else:
                parent.grad = (child.grad * -np.sin(parent.data))
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def sigmoid(parent: Tensor):
        # Make sure any input is a Tensor
        parent = Tensor.as_Tensor(parent)
        # The raw tensor data, with sigmoid applied
        child_data = 1 / (1 + np.exp(-parent.data))

        # If parent of output requires gradient, child requires gradient
        if parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: sigmoid({parents[0]._id})")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if parent.grad is not None:
                # Eventually look into caching this value during the forward pass
                parent.grad += child.grad * child.data * (1 - child.data)
            else:
                parent.grad = child.grad * child.data * (1 - child.data)
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def ReLU(parent: Tensor):
        # Make sure any input is a Tensor
        parent = Tensor.as_Tensor(parent)
        # The raw tensor data, with ReLU applied
        child_data = np.maximum(0, parent.data)

        # If parent of output requires gradient, child requires gradient
        if parent.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [parent]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: ReLU({parents[0]._id})")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
                
            grad_mask = (parent.data > 0).astype(float)
            if parent.grad is not None:
                parent.grad += child.grad * grad_mask
            else:
                parent.grad = child.grad * grad_mask
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
        
    def transpose(self):
        # The raw tensor data, with cos applied
        child_data = self.data.T

        # If parent of output requires gradient, child requires gradient
        if self.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [self]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: tranpose({parents[0]._id})")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if self.grad is not None:
                self.grad += child.grad.T
            else:
                self.grad = child.grad.T
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def __getitem__(self, idx):
        # The raw tensor data, indexed at idx
        child_data = self.data[idx]

        # If parent of output requires gradient, child requires gradient
        if self.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
        
        # Setting the new child Tensor's parent
        parents = [self]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id}[{idx}]")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if self.grad is None:
                self.grad = np.zeros_like(self.data)

            self.grad[idx] += child.grad
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
    def sum(self, axis: None | int | tuple):
        # Raw numpy data summed up
        child_data = self.data.sum(axis=axis)

        # If parent of output requires gradient, child requires gradient
        if self.requires_gradient:
            requires_gradient = True
        else:
            requires_gradient = False
        
        child = Tensor(child_data, requires_gradient=requires_gradient)
                
        # Setting the new child Tensor's parent
        parents = [self]
        child.set_parents(parents)
        
        if DEBUG:
            print(f"Child: {child._id} produced from: {parents[0]._id}.sum()")
        
        # Setting the child Tensor's backward prop rule
        def _backward():
            if DEBUG: 
                print(f"Propogating gradient from {child._id} to {[parent._id for parent in child._parents]} ")
            
            if axis == None:
                if self.grad is not None:
                    self.grad += np.ones_like(self.data) * child.grad
                else:
                    self.grad = np.ones_like(self.data) * child.grad
            else:
                if isinstance(axis, int):
                    axis_tuple = (axis,)
                else:
                    axis_tuple = axis
                child_grad_reshaped = child.grad
                for axs in sorted(axis_tuple):
                    child_grad_reshaped = np.expand_dims(child_grad_reshaped, axis=axs)
                child_grad_broadcasted = np.broadcast_to(child_grad_reshaped, self.shape)
                
                if self.grad is not None:
                    self.grad += child_grad_broadcasted
                else:
                    self.grad = child_grad_broadcasted
                
                
            if SUPER_DEBUG:
                print(f"New parent gradients: {[p.grad for p in parents]}")
                                
        child._backward = _backward
        
        return child
    
        
    
    